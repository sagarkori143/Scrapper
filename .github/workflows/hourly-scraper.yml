name: Enhanced Hourly Job Scraper

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour at minute 0
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours timeout to account for rate limiting
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        echo "ðŸ“¦ Installing Python dependencies..."
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        echo "ðŸŒ Installing browser dependencies..."
        playwright install --with-deps chromium
        
    - name: Create .env file
      run: |
        echo "ðŸ” Setting up environment variables..."
        echo "GOOGLE_API_KEY=${{ secrets.GOOGLE_API_KEY }}" > .env
        
    - name: Create necessary directories
      run: |
        echo "ðŸ“ Creating directories..."
        mkdir -p data
        mkdir -p results
        
    - name: Run enhanced job scraper
      run: |
        echo "ðŸš€ Starting enhanced hourly job scraping with intelligent rate limiting..."
        echo "â° Current time: $(date '+%Y-%m-%d %H:%M:%S UTC')"
        echo "ðŸ” Mode: Enhanced extraction (job IDs, URLs, descriptions, requirements, salary)"
        echo "ðŸ“Š Target: All companies with comprehensive data extraction"
        echo "âš¡ Rate limiting: Respecting Gemini API quotas for reliable operation"
        echo "ðŸ• Expected duration: 2-4 hours depending on rate limits"
        echo "================================"
        python scrapper.py
        
    - name: Generate scraping report
      run: |
        echo "ðŸ“ Generating scraping report..."
        echo "ðŸ“Š Jobs scraped: $(find data/ -name '*.json' -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo '0')"
        echo "ðŸ“ Data files created: $(find data/ -name '*.json' | wc -l)"
        echo "ðŸ’¾ Total data size: $(du -sh data/ 2>/dev/null | cut -f1 || echo '0B')"
        ls -la data/ || echo "No data directory found"
        
    - name: Upload job data artifacts
      uses: actions/upload-artifact@v4
      with:
        name: enhanced-job-data-${{ github.run_number }}
        path: |
          data/
          results/
        retention-days: 30
        
    - name: Commit and push changes
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        echo "ðŸ’¾ Committing scraped data..."
        git config --local user.email "action@github.com"
        git config --local user.name "Enhanced Job Scraper"
        git add data/ results/
        if git diff --staged --quiet; then
          echo "âœ… No new data to commit"
        else
          git commit -m "Enhanced job scraping - $(date '+%Y-%m-%d %H:%M:%S UTC')"
          git push
          echo "âœ… Data committed and pushed successfully"
        fi
