name: Daily Enhanced Job Scraper

on:
  schedule:
    # Run daily at 6:00 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      enhanced_mode:
        description: 'Enable enhanced extraction (job descriptions, IDs, etc.)'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  comprehensive-scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours timeout for comprehensive scraping
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        playwright install --with-deps chromium
        
    - name: Create .env file
      run: |
        echo "GOOGLE_API_KEY=${{ secrets.GOOGLE_API_KEY }}" > .env
        
    - name: Create necessary directories
      run: |
        mkdir -p data
        mkdir -p results
        mkdir -p configs
        
    - name: Run comprehensive enhanced job scraper
      run: |
        echo "Starting comprehensive job scraping for all 115 companies..."
        python scrapper.py
        
    - name: Generate summary report
      run: |
        echo "# Daily Job Scraping Report - $(date '+%Y-%m-%d')" > scraping_report.md
        echo "" >> scraping_report.md
        echo "## Summary" >> scraping_report.md
        echo "- **Date**: $(date '+%Y-%m-%d %H:%M:%S UTC')" >> scraping_report.md
        echo "- **Companies Processed**: $(ls data/*.json 2>/dev/null | wc -l)" >> scraping_report.md
        echo "- **CSV Files Generated**: $(ls results/*.csv 2>/dev/null | wc -l)" >> scraping_report.md
        echo "- **Total Job Records**: $(find data/ -name '*.json' -exec jq '.total_jobs // 0' {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')" >> scraping_report.md
        echo "" >> scraping_report.md
        echo "## Files Generated" >> scraping_report.md
        echo "### JSON Data Files:" >> scraping_report.md
        ls -la data/*.json 2>/dev/null | awk '{print "- " $9 " (" $5 " bytes)"}' >> scraping_report.md || echo "- No JSON files generated" >> scraping_report.md
        echo "" >> scraping_report.md
        echo "### CSV Result Files:" >> scraping_report.md
        ls -la results/*.csv 2>/dev/null | awk '{print "- " $9 " (" $5 " bytes)"}' >> scraping_report.md || echo "- No CSV files generated" >> scraping_report.md
        
    - name: Upload comprehensive job data
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-job-data-${{ github.run_number }}
        path: |
          data/
          results/
          configurations.json
          scraping_report.md
        retention-days: 90
        
    - name: Upload scraping report
      uses: actions/upload-artifact@v3
      with:
        name: scraping-report-${{ github.run_number }}
        path: scraping_report.md
        retention-days: 30
        
    - name: Commit and push all changes
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action - Daily Scraper"
        git add data/ results/ configurations.json scraping_report.md
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          echo "Committing changes..."
          git commit -m "ü§ñ Daily comprehensive job scraping - $(date '+%Y-%m-%d %H:%M:%S UTC')

          üìä Summary:
          - Companies: $(ls data/*.json 2>/dev/null | wc -l)
          - Jobs: $(find data/ -name '*.json' -exec jq '.total_jobs // 0' {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
          - Enhanced extraction: ‚úÖ
          
          üîç Enhanced data includes job IDs, URLs, descriptions, requirements, salary info, and more!"
          git push
        fi
        
    - name: Create Issue on Failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'üö® Daily Job Scraper Failed - ' + new Date().toISOString().split('T')[0],
            body: `## Job Scraper Failure Report
            
            **Date**: ${new Date().toISOString()}
            **Workflow**: Daily Enhanced Job Scraper
            **Run ID**: ${context.runId}
            
            ### Error Details
            The daily job scraping workflow has failed. Please check the logs for more details.
            
            ### Action Required
            1. Check the workflow logs: [View Run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            2. Verify API keys and configurations
            3. Check for website structure changes
            4. Review error messages and fix issues
            
            ### Quick Links
            - [Workflow File](/.github/workflows/daily-scraper.yml)
            - [Main Scraper](./scrapper.py)
            - [Configurations](./configurations.json)
            `,
            labels: ['bug', 'automation', 'scraper-failure']
          })
