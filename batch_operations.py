"""
Batch operations module for the intelligent job scraper.
Handles batch processing of multiple companies for scouting and scraping.
"""

from config import COMPANIES_FILE, CONFIGURATIONS_FILE, RESULTS_DIR, DATA_DIR
from utils import load_companies, ensure_directories, get_company_configuration
from web_scraper import scout_mode, scrape_mode
from data_storage import save_job_data


def batch_scout_mode(companies_file: str = COMPANIES_FILE):
    """
    Scout mode for all companies in the companies file.
    """
    print("ğŸš€ Running Batch Scout Mode for all companies...")
    companies = load_companies(companies_file)
    
    if not companies:
        return
    
    successful_scouts = 0
    failed_scouts = 0
    
    for company in companies:
        company_name = company.get('name', 'Unknown')
        career_url = company.get('career_url', '')
        
        if not career_url:
            print(f"ğŸ”´ No career URL found for {company_name}. Skipping...")
            failed_scouts += 1
            continue
        
        print(f"\n{'='*60}")
        print(f"ğŸ¢ Processing: {company_name}")
        print(f"ğŸ”— URL: {career_url}")
        print(f"{'='*60}")
        
        selectors = scout_mode(career_url, company_name)
        if selectors:
            successful_scouts += 1
        else:
            failed_scouts += 1
    
    print(f"\nğŸ“Š Batch Scout Summary:")
    print(f"âœ… Successful: {successful_scouts}")
    print(f"ğŸ”´ Failed: {failed_scouts}")
    print(f"ğŸ“ Configurations saved in: {CONFIGURATIONS_FILE}")


def batch_scrape_mode(companies_file: str = COMPANIES_FILE, enhanced: bool = True):
    """
    Scrape mode for all companies in the companies file with enhanced extraction.
    """
    extraction_type = "Enhanced" if enhanced else "Basic"
    print(f"ğŸš€ Running Batch {extraction_type} Scrape Mode for all companies...")
    companies = load_companies(companies_file)
    
    if not companies:
        return
    
    ensure_directories()
    successful_scrapes = 0
    failed_scrapes = 0
    total_jobs = 0
    
    for company in companies:
        company_name = company.get('name', 'Unknown')
        career_url = company.get('career_url', '')
        
        if not career_url:
            print(f"ğŸ”´ No career URL found for {company_name}. Skipping...")
            failed_scrapes += 1
            continue
        
        print(f"\n{'='*60}")
        print(f"ğŸ¢ Processing: {company_name}")
        print(f"ğŸ”— URL: {career_url}")
        print(f"ğŸ“Š Extraction: {extraction_type}")
        print(f"{'='*60}")
        
        # Get configuration
        selectors = get_company_configuration(company_name)
        if not selectors:
            print(f"ğŸ”´ No configuration found for {company_name}. Skipping...")
            failed_scrapes += 1
            continue
        
        jobs = scrape_mode(career_url, company_name, selectors, extract_full_details=enhanced)
        if jobs:
            # Save both CSV and JSON formats
            save_job_data(company_name, jobs, career_url)
            successful_scrapes += 1
            total_jobs += len(jobs)
            print(f"âœ… Found {len(jobs)} jobs for {company_name}")
        else:
            failed_scrapes += 1
            print(f"ğŸ”´ No jobs found for {company_name}")
    
    print(f"\nğŸ“Š Batch {extraction_type} Scrape Summary:")
    print(f"âœ… Successful companies: {successful_scrapes}")
    print(f"ğŸ”´ Failed companies: {failed_scrapes}")
    print(f"ğŸ’¼ Total jobs found: {total_jobs}")
    print(f"ğŸ“ CSV Results saved in: {RESULTS_DIR}")
    print(f"ğŸ“„ JSON Data saved in: {DATA_DIR}")


def intelligent_scrape_all(enhanced: bool = True):
    """
    Main intelligent workflow: Scout companies without configs, then scrape all with enhanced extraction.
    """
    extraction_type = "Enhanced" if enhanced else "Basic"
    print(f"ğŸš€ Starting Intelligent {extraction_type} Job Scraping Workflow")
    print("=" * 60)
    
    # Load companies
    companies = load_companies()
    if not companies:
        print("ğŸ”´ No companies found. Please check your companies.json file.")
        return
    
    ensure_directories()
    
    # Statistics
    total_companies = len(companies)
    companies_needing_scout = 0
    companies_with_config = 0
    successful_scrapes = 0
    failed_scrapes = 0
    total_jobs = 0
    
    print(f"ğŸ“Š Found {total_companies} companies to process")
    
    # Phase 1: Check which companies need scouting
    print("\nğŸ” Phase 1: Checking existing configurations...")
    for company in companies:
        company_name = company.get('name', 'Unknown')
        existing_config = get_company_configuration(company_name)
        
        if existing_config:
            companies_with_config += 1
            print(f"âœ… {company_name}: Configuration exists")
        else:
            companies_needing_scout += 1
            print(f"ğŸ” {company_name}: Needs scouting")
    
    print(f"\nğŸ“ˆ Configuration Status:")
    print(f"   âœ… Companies with configs: {companies_with_config}")
    print(f"   ğŸ” Companies needing scout: {companies_needing_scout}")
    
    # Phase 2: Scout companies that need it
    if companies_needing_scout > 0:
        print(f"\nğŸ•µï¸â€â™‚ï¸ Phase 2: Scouting {companies_needing_scout} companies...")
        scout_success = 0
        scout_failed = 0
        
        for company in companies:
            company_name = company.get('name', 'Unknown')
            career_url = company.get('career_url', '')
            
            # Skip if already has configuration
            if get_company_configuration(company_name):
                continue
                
            if not career_url:
                print(f"ğŸ”´ No career URL found for {company_name}. Skipping...")
                scout_failed += 1
                continue
            
            print(f"\n{'='*50}")
            print(f"ğŸ•µï¸â€â™‚ï¸ Scouting: {company_name}")
            print(f"ğŸ”— URL: {career_url}")
            print(f"{'='*50}")
            
            selectors = scout_mode(career_url, company_name)
            if selectors:
                scout_success += 1
                print(f"âœ… Successfully scouted {company_name}")
            else:
                scout_failed += 1
                print(f"ğŸ”´ Failed to scout {company_name}")
        
        print(f"\nğŸ“Š Scouting Summary:")
        print(f"   âœ… Successful: {scout_success}")
        print(f"   ğŸ”´ Failed: {scout_failed}")
    
    # Phase 3: Scrape all companies with enhanced extraction
    print(f"\nâš¡ Phase 3: Scraping jobs from all companies ({extraction_type} mode)...")
    
    for company in companies:
        company_name = company.get('name', 'Unknown')
        career_url = company.get('career_url', '')
        
        if not career_url:
            print(f"ğŸ”´ No career URL found for {company_name}. Skipping...")
            failed_scrapes += 1
            continue
        
        # Get configuration
        selectors = get_company_configuration(company_name)
        if not selectors:
            print(f"ğŸ”´ No configuration available for {company_name}. Skipping...")
            failed_scrapes += 1
            continue
        
        print(f"\n{'='*50}")
        print(f"âš¡ Scraping: {company_name}")
        print(f"ğŸ”— URL: {career_url}")
        print(f"ğŸ“Š Mode: {extraction_type}")
        print(f"{'='*50}")
        
        jobs = scrape_mode(career_url, company_name, selectors, extract_full_details=enhanced)
        if jobs:
            # Save both CSV and JSON formats
            save_job_data(company_name, jobs, career_url)
            successful_scrapes += 1
            total_jobs += len(jobs)
            print(f"âœ… Found {len(jobs)} jobs for {company_name}")
        else:
            failed_scrapes += 1
            print(f"ğŸ”´ No jobs found for {company_name}")
    
    # Final Summary
    print(f"\n{'='*60}")
    print(f"ğŸ‰ FINAL {extraction_type.upper()} EXTRACTION SUMMARY")
    print(f"{'='*60}")
    print(f"ğŸ“Š Total companies processed: {total_companies}")
    print(f"âœ… Successful scrapes: {successful_scrapes}")
    print(f"ğŸ”´ Failed scrapes: {failed_scrapes}")
    print(f"ğŸ’¼ Total jobs found: {total_jobs}")
    print(f"ğŸ“ CSV Results saved in: {RESULTS_DIR}")
    print(f"ğŸ“„ JSON Data saved in: {DATA_DIR}")
    print(f"âš™ï¸  Configurations saved in: {CONFIGURATIONS_FILE}")
    if enhanced:
        print(f"ğŸ” Enhanced data includes: Job IDs, URLs, Descriptions, Requirements, etc.")
    print(f"{'='*60}")
